---
layout:     post
title:   CEPH安装(一)
subtitle:   docker 模式
date:       2018-03-21
author:     AnberLu
header-img: img/post-bg-night.jpg
catalog: true
tags:
    - CEPH
    - 存储
    - 开源框架
---

# CEPH

CEPH是什么就不说了，只要记住它可以提供一套统一的存储架构，比如对象存储，文件存储系统和块存储系统，但是目前而言，大家对ceph的应用，大部分集中在块存储和对象存储中，而文件系统由于目前仅支持一个pool，同时mds的性能并没有做到非常的优秀，为此尚处于优化过程中。下面主要记录CEPH DOCKER的部署方式，使用的镜像为ceph/daemon

整个CEPH的核心，就是监控器MON，存储单元OSD，数据管理MGR（该模块可以提供ceph自带的dashboard,  当然也可以使用prometheus等和grafana来定制化监控项）。其余的都是根据自己的使用场景进行增减。

## 安装
首先是对ceph安装版本的一些简要介绍：
测试版本为：ceph version 12.2.2 (cf0baeeeeba3b47f9427c6c97e2144b094b7e5ba) luminous (stable)
采用磁盘挂载的方式进行部署OSD，为此需要先做一些准备工作，比如磁盘的提供。

```
1）为ceph osd提供裸盘
    卸载：umount  /dev/sde1
    格式化： mkfs.ext4 /dev/sde1
    删除分区： fdisk  /dev/sde
             查看： p
             删除： d
             保存： w

   注：如果在卸载盘时发现磁盘忙，被user占用，执行：fuser -kim /dev/sde1
       这里还需要注意的是，磁盘的分区一定要在数据格式化之后再进行删除，我想其中的道理大家都应该明白的。

2) 监控器Mon的安装

这里采用了主机网络，对于公网的解释，在CEPH里面存在两套网络，一个是集群网络，一个是公网，集群网络是为了让集群内部各模块间进行通信，公网是为了让外部与集群进行交互，两套网络实现了一定程度上的安全隔离。
docker run -d --net=host --name mon3 \
-v /ceph/servers/data/etc/ceph:/etc/ceph \
-v /ceph/servers/data/var/lib/ceph/:/var/lib/ceph/ \
-e MON_IP=172.19.182.87 \
-e CEPH_PUBLIC_NETWORK=172.19.182.0/24 \
ceph/daemon  mon

3) 存储模块Osd的安装

还是采用主机host网络模式，并且赋予特权，给予特权其实是因为偷懒，当然该部分还没有对osd进行cpu核的绑定来节省cpu资源的开销，也没有指定内存，未来会进行该部分的专门分析。

创建2个osd：
docker run -d --net=host --name osd0 \
--pid=host \
--privileged=true \
-e CLUSTER=ceph -e WEIGHT=1.0 -e MON_NAME=mon3 -e MON_IP=172.19.182.87  \
-v /ceph/servers/data/etc/ceph:/etc/ceph \
-v /ceph/servers/data/var/lib/ceph/:/var/lib/ceph/ \
-v /dev/:/dev/ \
-e OSD_TYPE=disk \
-e OSD_FORCE_ZAP=1 \
-e OSD_DEVICE=/dev/sde \
ceph/daemon osd

docker run -d --net=host --name osd2 \
--pid=host \
--privileged=true \
-e CLUSTER=ceph -e WEIGHT=1.0 -e MON_NAME=mon3 -e MON_IP=172.19.182.87  \
-v /ceph/servers/data/etc/ceph:/etc/ceph \
-v /ceph/servers/data/var/lib/ceph/:/var/lib/ceph/ \
-v /dev/:/dev/ \
-e OSD_TYPE=disk \
-e OSD_FORCE_ZAP=1 \
-e OSD_DEVICE=/dev/sdf \
ceph/daemon osd

其他两个类似。


4) 元数据管理器Mds

该部分主要对于使用文件存储系统而言的，如果没有使用文件存储系统，该部分可以忽略，这里也是走主机网络，同时CEPHFS_CREATE=1,表示自动创建默认的文件存储系统资源池
docker run -d --net=host \
-e CLUSTER=ceph -e MON_NAME=mon3 -e MON_IP=172.19.182.87  \
-v /ceph/servers/data/etc/ceph:/etc/ceph \
-v /ceph/servers/data/var/lib/ceph/:/var/lib/ceph/ \
-e CEPHFS_CREATE=1 \
ceph/daemon mds

5) 网关Rgw

该部分主要针对对象存储而言，这里设置网关后，便可以使用s3来进行数据的交互。
以156作为客户端

rgw的启动命令：
docker run -d  --net=host  \
-v /export/App/ceph/client/data/etc/ceph:/etc/ceph \
-v /export/App/ceph/client/data/var/lib/ceph/:/var/lib/ceph/ \
-p 8080:8080 \
--name rgw1  \
ceph/daemon rgw

测试是否成功：
curl -H "Content-Type: application/json" "http://127.0.0.1:8080"

下面来使用s3来测试下对象存储：

（1）python客户端：
软件下载与安装：
wget "https://gist.githubusercontent.com/kairen/e0dec164fa6664f40784f303076233a5/raw/33add5a18cb7d6f18531d8d481562d017557747c/s3client"

chmod u+x s3client
sudo pip install boto

创建使用者：
docker exec -ti rgw1 radosgw-admin user create --uid="test" --display-name="I'm Test account" --email="test@example.com"

创建记录文件s3key.sh：
export S3_ACCESS_KEY="GB7YEHT26Q55PIDQGF8R"
export S3_SECRET_KEY="JGUj3jMygrGSdmqrYV3PopxbmNAzlQdPBthZtE4n"
export S3_HOST="127.0.0.1"
export S3_PORT="8080"

source  s3key.sh

（2）执行s3命令：
列出档案：
./s3client list
建立档案：
./s3client create files
上传文件：
./s3client upload files s3key.sh /
列出文件：
./s3client list files
下载文件：
./s3client download files s3key.sh

（3）编写python脚本：
import boto
import boto.s3.connection
access_key = 'GB7YEHT26Q55PIDQGF8R'
secret_key = 'JGUj3jMygrGSdmqrYV3PopxbmNAzlQdPBthZtE4n'
conn = boto.connect_s3(
aws_access_key_id = access_key,
aws_secret_access_key = secret_key,
host = '172.28.170.156',port=8080,
is_secure=False,
calling_format = boto.s3.connection.OrdinaryCallingFormat(),
)
bucket = conn.create_bucket('my-new-bucket')
for bucket in conn.get_all_buckets():
        print "{name}\t{created}".format(
                name = bucket.name,
                created = bucket.creation_date,
)

执行脚本：
[root@A01-R14-I170-156 s3]# python s3_test.py                 
files   2018-02-26T09:27:42.256Z
my-new-bucket   2018-02-26T09:43:25.018Z


6) 块存储Rbd-mirror

该部分主要是开启rbd mirror功能，使用的场景一般是考虑容灾而进行的数据备份

modprobe rbd  # 该步的含义其实是将rbd加载到内核，可以使用lsmod |grep rbd 来查看是否加载，如果没有加载，后面使用rbd时，可能会出现input/output error等错误
docker run -d --net=host \
-e CLUSTER=ceph -e MON_NAME=mon3 -e MON_IP=172.19.182.87  \
-v /ceph/servers/data/etc/ceph:/etc/ceph \
-v /ceph/servers/data/var/lib/ceph/:/var/lib/ceph/ \
--privileged   \
-v /dev:/dev \
ceph/daemon rbd_mirror


7) 数据管理mgr

该部分主要是ceph给予用户的一个数据展现和部分管理，你可以通过配置mgr来查看你的监控看板dashboard。如果想要定制化的监控项，可以使用其他的module来实现，具体可以ceph mgr module  ls来查看。

docker run -d --net=host \
-e CLUSTER=ceph -e MON_NAME=mon3 -e MON_IP=172.19.182.87  \
-v /ceph/servers/data/etc/ceph:/etc/ceph \
-v /ceph/servers/data/var/lib/ceph/:/var/lib/ceph/ \
-e MGR_NAME=mgr1 \
-e CEPH_PUBLIC_NETWORK=172.19.182.0/24 \
ceph/daemon mgr


查看mgr状态：
docker exec -it mon3  ceph mgr dump

enable dashboard：
docker exec -it 62dcec790941   ceph mgr module enable dashboard


dashboard地址： http://172.19.182.87:7000/


8) 集群测试
docker exec mon3  ceph -s
docker exec mon3  ceph -w
docker exec mon3  ceph osd tree
```

## 测试
```
1) 重新设定crushmap规则

发现无法写数据，原因是pg 状态为peered以及Undersized,原因在于pg的副本数无法得到满足，crushmap的规则应该变为副本数可以精确到osd。
为此，首先查看crushmap
[root@BJYFB3-Druid-18287 servers]# docker exec -it mon3  ceph osd getcrushmap -o /var/lib/ceph/cmap
[root@BJYFB3-Druid-18287 servers]# docker exec -it mon3  crushtool -d /var/lib/ceph/cmap -o /var/lib/ceph/map.txt 
[root@BJYFB3-Druid-18287 servers]# cat map.txt
规则为：
# rules
rule replicated_rule {
        id 0
        type replicated
        min_size 1
        max_size 10
        step take default
        step chooseleaf firstn 0 type host
        step emit
}

修改为： step choose firstn 0 type osd

更新crushmap:
[root@BJYFB3-Druid-18287 servers]# docker exec -it mon3  crushtool -c /var/lib/ceph/map.txt -o /var/lib/ceph/cmap-complied
[root@BJYFB3-Druid-18287 servers]# docker exec -it mon3  ceph osd setcrushmap -i /var/lib/ceph/cmap-complied


2）基本操作
创建pool
docker exec -it mon3 ceph osd pool create test_pool 128 128 replicated
配额设置
docker exec -it mon3 ceph osd pool set-quota test_pool max_objects 10000
docker exec -it mon3 ceph osd pool set-quota test_pool max_bytes 102400
重命名
docker exec -it mon3 ceph osd pool rename   test_pool  test
快照
docker exec -it mon3 ceph osd pool mksnap test testsnap
查看
docker exec -it mon3 ceph osd lspools
创建2GB的块设备镜像
docker exec -it mon3   rbd create -p test --size 100 kjh
查看镜像
docker exec -it mon3  rbd info -p test  --image kjh
删除镜像
docker exec -it mon3 rbd rm test/kjh
创建对象
docker exec -it mon3  rados create test-object -p test
查看对象
docker exec -it mon3  rados -p test ls
查看对象位置
docker exec -it mon3  ceph osd map test test-object
向pool中上传对象内容
docker exec -it mon3  rados -p test  put test-object  /var/lib/ceph/map.txt

查看pg映射关系信息
docker exec -it mon3  ceph pg ls
查看pg的状态信息：
（1）docker exec -it mon3 ceph pg dump_stuck unclean
（2）docker exec -it mon3 ceph pg dump_stuck inactive
（3）docker exec -it mon3 ceph pg dump_stuck stale
  相关解释：
    Inactive （不活跃）归置组不能处理读写，因为它们在等待一个有最新数据的 OSD 复活且进入集群。
    Unclean （不干净）归置组含有复制数未达到期望数量的对象，它们应该在恢复中。
    Stale （不新鲜）归置组处于未知状态：存储它们的 OSD 有段时间没向监视器报告了（由 mon_osd_report_timeout 配置）。
    可用格式有 plain （默认）和 json 。阀值定义的是，归置组被认为卡住前等待的最小时间（默认 300 秒）


2） 其他性能测试
写操作，并发100,时间60秒，目标池 test：
docker exec -it mymon  rados bench 60 write  rand -t 100 -b 4K -p test

```

## 参考：

- [ceph对象存储](https://kairen.github.io/2016/02/11/ceph/deploy/ceph-docker/)
- [ceph官网总体架构](http://docs.ceph.com/docs/jewel/architecture/)
- [ceph数据恢复](http://www.csdn.net/article/2014-04-08/2819192-ceph-swift-on-openstack-m/2)
- [rbd内核问题：/sbin/modinfo: not found /sbin/modprobe: not foundrbd: failed to load rbd kernel module](http://blog.163.com/digoal@126/blog/static/1638770402014112325944867/)
- [osd init failed (36) File name too long](http://blog.csdn.net/styshoo/article/details/60951468)
