---
layout:     post
title:      ReactiveCocoa 进阶
subtitle:   函数式编程框架 ReactiveCocoa 进阶
date:       2017-01-06
author:     BY
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - iOS
    - ReactiveCocoa
    - 函数式编程
    - 开源框架
---
# CEPH

#### 安装
测试版本为：ceph version 12.2.2 (cf0baeeeeba3b47f9427c6c97e2144b094b7e5ba) luminous (stable)

```
1）为ceph osd提供裸盘
    卸载：umount  /dev/sde1
    格式化： mkfs.ext4 /dev/sde1
    删除分区： fdisk  /dev/sde
             查看： p
             删除： d
             保存： w

   注：如果在卸载盘时发现磁盘忙，被user占用，执行：fuser -kim /dev/sde1

2) mon
docker run -d --net=host --name mon3 \
-v /ceph/servers/data/etc/ceph:/etc/ceph \
-v /ceph/servers/data/var/lib/ceph/:/var/lib/ceph/ \
-e MON_IP=172.19.182.87 \
-e CEPH_PUBLIC_NETWORK=172.19.182.0/24 \
ceph/daemon  mon

3) osd

创建4个osd：
docker run -d --net=host --name osd0 \
--pid=host \
--privileged=true \
-e CLUSTER=ceph -e WEIGHT=1.0 -e MON_NAME=mon3 -e MON_IP=172.19.182.87  \
-v /ceph/servers/data/etc/ceph:/etc/ceph \
-v /ceph/servers/data/var/lib/ceph/:/var/lib/ceph/ \
-v /dev/:/dev/ \
-e OSD_TYPE=disk \
-e OSD_FORCE_ZAP=1 \
-e OSD_DEVICE=/dev/sde \
ceph/daemon osd

docker run -d --net=host --name osd2 \
--pid=host \
--privileged=true \
-e CLUSTER=ceph -e WEIGHT=1.0 -e MON_NAME=mon3 -e MON_IP=172.19.182.87  \
-v /ceph/servers/data/etc/ceph:/etc/ceph \
-v /ceph/servers/data/var/lib/ceph/:/var/lib/ceph/ \
-v /dev/:/dev/ \
-e OSD_TYPE=disk \
-e OSD_FORCE_ZAP=1 \
-e OSD_DEVICE=/dev/sdf \
ceph/daemon osd

其他两个类似。


4) mds
docker run -d --net=host \
-e CLUSTER=ceph -e MON_NAME=mon3 -e MON_IP=172.19.182.87  \
-v /ceph/servers/data/etc/ceph:/etc/ceph \
-v /ceph/servers/data/var/lib/ceph/:/var/lib/ceph/ \
-e CEPHFS_CREATE=1 \
ceph/daemon mds

5)rgw
以156作为客户端

rgw的启动命令：
docker run -d  --net=host  \
-v /export/App/ceph/client/data/etc/ceph:/etc/ceph \
-v /export/App/ceph/client/data/var/lib/ceph/:/var/lib/ceph/ \
-p 8080:8080 \
--name rgw1  \
ceph/daemon rgw

测试：
curl -H "Content-Type: application/json" "http://127.0.0.1:8080"

python客户端：
软件下载与安装：
wget "https://gist.githubusercontent.com/kairen/e0dec164fa6664f40784f303076233a5/raw/33add5a18cb7d6f18531d8d481562d017557747c/s3client"

chmod u+x s3client
sudo pip install boto

创建使用者：
docker exec -ti rgw1 radosgw-admin user create --uid="test" --display-name="I'm Test account" --email="test@example.com"

创建记录文件s3key.sh：
export S3_ACCESS_KEY="GB7YEHT26Q55PIDQGF8R"
export S3_SECRET_KEY="JGUj3jMygrGSdmqrYV3PopxbmNAzlQdPBthZtE4n"
export S3_HOST="127.0.0.1"
export S3_PORT="8080"

source  s3key.sh

执行s3命令：
列出档案：
./s3client list
建立档案：
./s3client create files
上传文件：
./s3client upload files s3key.sh /
列出文件：
./s3client list files
下载文件：
./s3client download files s3key.sh

编写python脚本：
import boto
import boto.s3.connection
access_key = 'GB7YEHT26Q55PIDQGF8R'
secret_key = 'JGUj3jMygrGSdmqrYV3PopxbmNAzlQdPBthZtE4n'
conn = boto.connect_s3(
aws_access_key_id = access_key,
aws_secret_access_key = secret_key,
host = '172.28.170.156',port=8080,
is_secure=False,
calling_format = boto.s3.connection.OrdinaryCallingFormat(),
)
bucket = conn.create_bucket('my-new-bucket')
for bucket in conn.get_all_buckets():
        print "{name}\t{created}".format(
                name = bucket.name,
                created = bucket.creation_date,
)

执行脚本：
[root@A01-R14-I170-156 s3]# python s3_test.py                 
files   2018-02-26T09:27:42.256Z
my-new-bucket   2018-02-26T09:43:25.018Z


6) rbd

modprobe rbd
docker run -d --net=host \
-e CLUSTER=ceph -e MON_NAME=mon3 -e MON_IP=172.19.182.87  \
-v /ceph/servers/data/etc/ceph:/etc/ceph \
-v /ceph/servers/data/var/lib/ceph/:/var/lib/ceph/ \
--privileged   \
-v /dev:/dev \
ceph/daemon rbd_mirror


7) mgr
docker run -d --net=host \
-e CLUSTER=ceph -e MON_NAME=mon3 -e MON_IP=172.19.182.87  \
-v /ceph/servers/data/etc/ceph:/etc/ceph \
-v /ceph/servers/data/var/lib/ceph/:/var/lib/ceph/ \
-e MGR_NAME=mgr1 \
-e CEPH_PUBLIC_NETWORK=172.19.182.0/24 \
ceph/daemon mgr


查看mgr状态：
docker exec -it mon3  ceph mgr dump

enable dashboard：
docker exec -it 62dcec790941   ceph mgr module enable dashboard


dashboard地址： http://172.19.182.87:7000/


8)test
docker exec mon3  ceph -s
docker exec mon3  ceph -w
docker exec mon3  ceph osd tree
```

#### 测试
```
1) 重新设定crushmap规则

发现无法写数据，原因是pg 状态为peered以及Undersized,原因在于pg的副本数无法得到满足，crushmap的规则应该变为副本数可以精确到osd。
为此，首先查看crushmap
[root@BJYFB3-Druid-18287 servers]# docker exec -it mon3  ceph osd getcrushmap -o /var/lib/ceph/cmap
[root@BJYFB3-Druid-18287 servers]# docker exec -it mon3  crushtool -d /var/lib/ceph/cmap -o /var/lib/ceph/map.txt 
[root@BJYFB3-Druid-18287 servers]# cat map.txt
规则为：
# rules
rule replicated_rule {
        id 0
        type replicated
        min_size 1
        max_size 10
        step take default
        step chooseleaf firstn 0 type host
        step emit
}

修改为： step choose firstn 0 type osd

更新crushmap:
[root@BJYFB3-Druid-18287 servers]# docker exec -it mon3  crushtool -c /var/lib/ceph/map.txt -o /var/lib/ceph/cmap-complied
[root@BJYFB3-Druid-18287 servers]# docker exec -it mon3  ceph osd setcrushmap -i /var/lib/ceph/cmap-complied


2）基本操作
创建pool
docker exec -it mon3 ceph osd pool create test_pool 128 128 replicated
配额设置
docker exec -it mon3 ceph osd pool set-quota test_pool max_objects 10000
docker exec -it mon3 ceph osd pool set-quota test_pool max_bytes 102400
重命名
docker exec -it mon3 ceph osd pool rename   test_pool  test
快照
docker exec -it mon3 ceph osd pool mksnap test testsnap
查看
docker exec -it mon3 ceph osd lspools
创建2GB的块设备镜像
docker exec -it mon3   rbd create -p test --size 100 kjh
查看镜像
docker exec -it mon3  rbd info -p test  --image kjh
删除镜像
docker exec -it mon3 rbd rm test/kjh
创建对象
docker exec -it mon3  rados create test-object -p test
查看对象
docker exec -it mon3  rados -p test ls
查看对象位置
docker exec -it mon3  ceph osd map test test-object
向pool中上传对象内容
docker exec -it mon3  rados -p test  put test-object  /var/lib/ceph/map.txt

查看pg映射关系信息
docker exec -it mon3  ceph pg ls
查看pg的状态信息：
（1）docker exec -it mon3 ceph pg dump_stuck unclean
（2）docker exec -it mon3 ceph pg dump_stuck inactive
（3）docker exec -it mon3 ceph pg dump_stuck stale
  相关解释：
    Inactive （不活跃）归置组不能处理读写，因为它们在等待一个有最新数据的 OSD 复活且进入集群。
    Unclean （不干净）归置组含有复制数未达到期望数量的对象，它们应该在恢复中。
    Stale （不新鲜）归置组处于未知状态：存储它们的 OSD 有段时间没向监视器报告了（由 mon_osd_report_timeout 配置）。
    可用格式有 plain （默认）和 json 。阀值定义的是，归置组被认为卡住前等待的最小时间（默认 300 秒）


11)add/remove osd时数据的变换
首先是针对remove的情况：
当osd服务逐出时，如果不涉及主osd，则数据原始映射保持不变，如果主osd被逐出，则从osd会成为主osd，如果所有相关osd都逐出，则会从选择其他osd作为主osd。

下面根据两种crushmap规则进行
(1-1) step choose firstn 0 type osd
查看pg的映射
    [root@BJYFB3-Druid-18287 servers]# docker exec -it mon3   ceph pg  map 4.6c
    osdmap e294 pg 4.6c (4.6c) -> up [2,0,4] acting [2,0,4]
测试：
    逐出osd.4
    [root@BJYFB3-Druid-18287 servers]# docker exec -it mon3   ceph osd out osd.4
    marked out osd.4.

    [root@BJYFB3-Druid-18287 servers]# docker exec -it mon3   ceph pg  map 4.6c
    osdmap e296 pg 4.6c (4.6c) -> up [2,0,5] acting [2,0,5]
    加入osd.4
    [root@BJYFB3-Druid-18287 servers]# docker exec -it mon3   ceph osd in osd.4 
    marked in osd.4.

    [root@BJYFB3-Druid-18287 servers]# docker exec -it mon3   ceph pg  map 4.6c
    osdmap e51 pg 4.6c (4.6c) -> up [2,0,4] acting [2,0,4]
    
    逐出osd.0:
    osdmap e149 pg 4.6c (4.6c) -> up [2,4,5] acting [2,4,5]
    逐出osd.2:
    osdmap e302 pg 4.6c (4.6c) -> up [4,5,1] acting [4,5,1]
    逐出osd.4:
    osdmap e304 pg 4.6c (4.6c) -> up [5,1,6] acting [5,1,6]
    加入osd.0:
    osdmap e306 pg 4.6c (4.6c) -> up [0,5,1] acting [0,5,1]
    加入osd.2:
    osdmap e308 pg 4.6c (4.6c) -> up [2,0,5] acting [2,0,5]
    加入osd.4:
    osdmap e159 pg 4.6c (4.6c) -> up [2,0,4] acting [2,0,4]


（1-2）step chooseleaf firstn 1 type host
查看pg的映射
    [root@BJYFB3-Druid-18287 servers]# docker exec -it mon3   ceph pg  map 4.6c
    osdmap e46 pg 4.6c (4.6c) -> up [2,0,1] acting [2,0,1]
测试：
    逐出osd.1，osd.0:
       osdmap e94 pg 4.6c (4.6c) -> up [2] acting [2,0,1]
    逐出主osd.2:
       osdmap e172 pg 4.6c (4.6c) -> up [4] acting [4,2,0]
    逐出osd.4:
       osdmap e176 pg 4.6c (4.6c) -> up [5] acting [5,4,2]
    逐出osd.5:
       osdmap e101 pg 4.6c (4.6c) -> up [6] acting [6,5,4]
    加入osd.0:
       osdmap e278 pg 4.6c (4.6c) -> up [0] acting [0,6,5]
    加入osd.2:
       osdmap e282 pg 4.6c (4.6c) -> up [2] acting [2,0,6]
    加入osd.5:
       osdmap e285 pg 4.6c (4.6c) -> up [2] acting [2,0,6]
    加入osd.4:
       osdmap e285 pg 4.6c (4.6c) -> up [2] acting [2,0,6] 
    加入osd.1:
       osdmap e291 pg 4.6c (4.6c) -> up [2] acting [2,0,6]


3） 其他性能测试
写操作，并发100,时间60秒，目标池 test：
docker exec -it mymon  rados bench 60 write  rand -t 100 -b 4K -p test

```

参考：

[ceph对象存储](https://kairen.github.io/2016/02/11/ceph/deploy/ceph-docker/)

[ceph官网总体架构](http://docs.ceph.com/docs/jewel/architecture/)

[ceph数据恢复](http://www.csdn.net/article/2014-04-08/2819192-ceph-swift-on-openstack-m/2)

[rbd内核问题：/sbin/modinfo: not found /sbin/modprobe: not foundrbd: failed to load rbd kernel module](http://blog.163.com/digoal@126/blog/static/1638770402014112325944867/)

[osd init failed (36) File name too long](http://blog.csdn.net/styshoo/article/details/60951468)
