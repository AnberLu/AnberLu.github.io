---
layout:     post
title:   CEPH安装(一)
subtitle:   docker 模式
date:       2018-03-21
author:     AnberLu
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - CEPH
    - 存储
    - 开源框架
---

# CEPH

这里主要记录CEPH DOCKER的形式来进行部署，使用的镜像为ceph/daemon

## 安装
测试版本为：ceph version 12.2.2 (cf0baeeeeba3b47f9427c6c97e2144b094b7e5ba) luminous (stable)

```
1）为ceph osd提供裸盘
    卸载：umount  /dev/sde1
    格式化： mkfs.ext4 /dev/sde1
    删除分区： fdisk  /dev/sde
             查看： p
             删除： d
             保存： w

   注：如果在卸载盘时发现磁盘忙，被user占用，执行：fuser -kim /dev/sde1

2) mon
docker run -d --net=host --name mon3 \
-v /ceph/servers/data/etc/ceph:/etc/ceph \
-v /ceph/servers/data/var/lib/ceph/:/var/lib/ceph/ \
-e MON_IP=172.19.182.87 \
-e CEPH_PUBLIC_NETWORK=172.19.182.0/24 \
ceph/daemon  mon

3) osd

创建4个osd：
docker run -d --net=host --name osd0 \
--pid=host \
--privileged=true \
-e CLUSTER=ceph -e WEIGHT=1.0 -e MON_NAME=mon3 -e MON_IP=172.19.182.87  \
-v /ceph/servers/data/etc/ceph:/etc/ceph \
-v /ceph/servers/data/var/lib/ceph/:/var/lib/ceph/ \
-v /dev/:/dev/ \
-e OSD_TYPE=disk \
-e OSD_FORCE_ZAP=1 \
-e OSD_DEVICE=/dev/sde \
ceph/daemon osd

docker run -d --net=host --name osd2 \
--pid=host \
--privileged=true \
-e CLUSTER=ceph -e WEIGHT=1.0 -e MON_NAME=mon3 -e MON_IP=172.19.182.87  \
-v /ceph/servers/data/etc/ceph:/etc/ceph \
-v /ceph/servers/data/var/lib/ceph/:/var/lib/ceph/ \
-v /dev/:/dev/ \
-e OSD_TYPE=disk \
-e OSD_FORCE_ZAP=1 \
-e OSD_DEVICE=/dev/sdf \
ceph/daemon osd

其他两个类似。


4) mds
docker run -d --net=host \
-e CLUSTER=ceph -e MON_NAME=mon3 -e MON_IP=172.19.182.87  \
-v /ceph/servers/data/etc/ceph:/etc/ceph \
-v /ceph/servers/data/var/lib/ceph/:/var/lib/ceph/ \
-e CEPHFS_CREATE=1 \
ceph/daemon mds

5)rgw
以156作为客户端

rgw的启动命令：
docker run -d  --net=host  \
-v /export/App/ceph/client/data/etc/ceph:/etc/ceph \
-v /export/App/ceph/client/data/var/lib/ceph/:/var/lib/ceph/ \
-p 8080:8080 \
--name rgw1  \
ceph/daemon rgw

测试：
curl -H "Content-Type: application/json" "http://127.0.0.1:8080"

python客户端：
软件下载与安装：
wget "https://gist.githubusercontent.com/kairen/e0dec164fa6664f40784f303076233a5/raw/33add5a18cb7d6f18531d8d481562d017557747c/s3client"

chmod u+x s3client
sudo pip install boto

创建使用者：
docker exec -ti rgw1 radosgw-admin user create --uid="test" --display-name="I'm Test account" --email="test@example.com"

创建记录文件s3key.sh：
export S3_ACCESS_KEY="GB7YEHT26Q55PIDQGF8R"
export S3_SECRET_KEY="JGUj3jMygrGSdmqrYV3PopxbmNAzlQdPBthZtE4n"
export S3_HOST="127.0.0.1"
export S3_PORT="8080"

source  s3key.sh

执行s3命令：
列出档案：
./s3client list
建立档案：
./s3client create files
上传文件：
./s3client upload files s3key.sh /
列出文件：
./s3client list files
下载文件：
./s3client download files s3key.sh

编写python脚本：
import boto
import boto.s3.connection
access_key = 'GB7YEHT26Q55PIDQGF8R'
secret_key = 'JGUj3jMygrGSdmqrYV3PopxbmNAzlQdPBthZtE4n'
conn = boto.connect_s3(
aws_access_key_id = access_key,
aws_secret_access_key = secret_key,
host = '172.28.170.156',port=8080,
is_secure=False,
calling_format = boto.s3.connection.OrdinaryCallingFormat(),
)
bucket = conn.create_bucket('my-new-bucket')
for bucket in conn.get_all_buckets():
        print "{name}\t{created}".format(
                name = bucket.name,
                created = bucket.creation_date,
)

执行脚本：
[root@A01-R14-I170-156 s3]# python s3_test.py                 
files   2018-02-26T09:27:42.256Z
my-new-bucket   2018-02-26T09:43:25.018Z


6) rbd

modprobe rbd
docker run -d --net=host \
-e CLUSTER=ceph -e MON_NAME=mon3 -e MON_IP=172.19.182.87  \
-v /ceph/servers/data/etc/ceph:/etc/ceph \
-v /ceph/servers/data/var/lib/ceph/:/var/lib/ceph/ \
--privileged   \
-v /dev:/dev \
ceph/daemon rbd_mirror


7) mgr
docker run -d --net=host \
-e CLUSTER=ceph -e MON_NAME=mon3 -e MON_IP=172.19.182.87  \
-v /ceph/servers/data/etc/ceph:/etc/ceph \
-v /ceph/servers/data/var/lib/ceph/:/var/lib/ceph/ \
-e MGR_NAME=mgr1 \
-e CEPH_PUBLIC_NETWORK=172.19.182.0/24 \
ceph/daemon mgr


查看mgr状态：
docker exec -it mon3  ceph mgr dump

enable dashboard：
docker exec -it 62dcec790941   ceph mgr module enable dashboard


dashboard地址： http://172.19.182.87:7000/


8)test
docker exec mon3  ceph -s
docker exec mon3  ceph -w
docker exec mon3  ceph osd tree
```

## 测试
```
1) 重新设定crushmap规则

发现无法写数据，原因是pg 状态为peered以及Undersized,原因在于pg的副本数无法得到满足，crushmap的规则应该变为副本数可以精确到osd。
为此，首先查看crushmap
[root@BJYFB3-Druid-18287 servers]# docker exec -it mon3  ceph osd getcrushmap -o /var/lib/ceph/cmap
[root@BJYFB3-Druid-18287 servers]# docker exec -it mon3  crushtool -d /var/lib/ceph/cmap -o /var/lib/ceph/map.txt 
[root@BJYFB3-Druid-18287 servers]# cat map.txt
规则为：
# rules
rule replicated_rule {
        id 0
        type replicated
        min_size 1
        max_size 10
        step take default
        step chooseleaf firstn 0 type host
        step emit
}

修改为： step choose firstn 0 type osd

更新crushmap:
[root@BJYFB3-Druid-18287 servers]# docker exec -it mon3  crushtool -c /var/lib/ceph/map.txt -o /var/lib/ceph/cmap-complied
[root@BJYFB3-Druid-18287 servers]# docker exec -it mon3  ceph osd setcrushmap -i /var/lib/ceph/cmap-complied


2）基本操作
创建pool
docker exec -it mon3 ceph osd pool create test_pool 128 128 replicated
配额设置
docker exec -it mon3 ceph osd pool set-quota test_pool max_objects 10000
docker exec -it mon3 ceph osd pool set-quota test_pool max_bytes 102400
重命名
docker exec -it mon3 ceph osd pool rename   test_pool  test
快照
docker exec -it mon3 ceph osd pool mksnap test testsnap
查看
docker exec -it mon3 ceph osd lspools
创建2GB的块设备镜像
docker exec -it mon3   rbd create -p test --size 100 kjh
查看镜像
docker exec -it mon3  rbd info -p test  --image kjh
删除镜像
docker exec -it mon3 rbd rm test/kjh
创建对象
docker exec -it mon3  rados create test-object -p test
查看对象
docker exec -it mon3  rados -p test ls
查看对象位置
docker exec -it mon3  ceph osd map test test-object
向pool中上传对象内容
docker exec -it mon3  rados -p test  put test-object  /var/lib/ceph/map.txt

查看pg映射关系信息
docker exec -it mon3  ceph pg ls
查看pg的状态信息：
（1）docker exec -it mon3 ceph pg dump_stuck unclean
（2）docker exec -it mon3 ceph pg dump_stuck inactive
（3）docker exec -it mon3 ceph pg dump_stuck stale
  相关解释：
    Inactive （不活跃）归置组不能处理读写，因为它们在等待一个有最新数据的 OSD 复活且进入集群。
    Unclean （不干净）归置组含有复制数未达到期望数量的对象，它们应该在恢复中。
    Stale （不新鲜）归置组处于未知状态：存储它们的 OSD 有段时间没向监视器报告了（由 mon_osd_report_timeout 配置）。
    可用格式有 plain （默认）和 json 。阀值定义的是，归置组被认为卡住前等待的最小时间（默认 300 秒）


2） 其他性能测试
写操作，并发100,时间60秒，目标池 test：
docker exec -it mymon  rados bench 60 write  rand -t 100 -b 4K -p test

```

## 参考：

- [ceph对象存储](https://kairen.github.io/2016/02/11/ceph/deploy/ceph-docker/)
- [ceph官网总体架构](http://docs.ceph.com/docs/jewel/architecture/)
- [ceph数据恢复](http://www.csdn.net/article/2014-04-08/2819192-ceph-swift-on-openstack-m/2)
- [rbd内核问题：/sbin/modinfo: not found /sbin/modprobe: not foundrbd: failed to load rbd kernel module](http://blog.163.com/digoal@126/blog/static/1638770402014112325944867/)
- [osd init failed (36) File name too long](http://blog.csdn.net/styshoo/article/details/60951468)
